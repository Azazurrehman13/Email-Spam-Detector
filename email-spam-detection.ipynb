{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":998616,"sourceType":"datasetVersion","datasetId":547699},{"sourceId":420830,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":342981,"modelId":364268},{"sourceId":420862,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":343002,"modelId":364287}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:15:27.136277Z","iopub.execute_input":"2025-06-01T11:15:27.136496Z","iopub.status.idle":"2025-06-01T11:15:27.140876Z","shell.execute_reply.started":"2025-06-01T11:15:27.136479Z","shell.execute_reply":"2025-06-01T11:15:27.140007Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 1. Read the dataset\ndf = pd.read_csv('/kaggle/input/email-spam-classification-dataset-csv/emails.csv')\n\n# 2. Load the dataset (view top rows & class balance)\nprint(df.head())\nprint(\"\\nClass Distribution:\\n\", df['spam'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:16:52.238280Z","iopub.execute_input":"2025-06-01T11:16:52.238984Z","iopub.status.idle":"2025-06-01T11:16:53.759346Z","shell.execute_reply.started":"2025-06-01T11:16:52.238961Z","shell.execute_reply":"2025-06-01T11:16:53.758613Z"}},"outputs":[{"name":"stdout","text":"  Email No.  the  to  ect  and  for  of    a  you  hou  ...  connevey  jay  \\\n0   Email 1    0   0    1    0    0   0    2    0    0  ...         0    0   \n1   Email 2    8  13   24    6    6   2  102    1   27  ...         0    0   \n2   Email 3    0   0    1    0    0   0    8    0    0  ...         0    0   \n3   Email 4    0   5   22    0    5   1   51    2   10  ...         0    0   \n4   Email 5    7   6   17    1    5   2   57    0    9  ...         0    0   \n\n   valued  lay  infrastructure  military  allowing  ff  dry  Prediction  \n0       0    0               0         0         0   0    0           0  \n1       0    0               0         0         0   1    0           0  \n2       0    0               0         0         0   0    0           0  \n3       0    0               0         0         0   0    0           0  \n4       0    0               0         0         0   1    0           0  \n\n[5 rows x 3002 columns]\n\nClass Distribution:\n spam\n0    5104\n1      57\n2       5\n4       3\n5       2\n3       1\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 3. Preprocess the dataset\ndef clean_text(text):\n    text = str(text)  # Ensure the input is a string\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # remove links\n    text = re.sub(r'\\W', ' ', text)                     # remove non-alphanumeric\n    text = re.sub(r'\\s+', ' ', text)                    # remove extra spaces\n    return text\n\n# Apply preprocessing\ndf['text'] = df['text'].apply(clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:19:56.991402Z","iopub.execute_input":"2025-06-01T11:19:56.991662Z","iopub.status.idle":"2025-06-01T11:19:57.007688Z","shell.execute_reply.started":"2025-06-01T11:19:56.991641Z","shell.execute_reply":"2025-06-01T11:19:57.007069Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Remove empty or whitespace-only strings after cleaning\ndf['text'] = df['text'].apply(clean_text)\ndf = df[df['text'].str.strip() != '']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:21:04.130042Z","iopub.execute_input":"2025-06-01T11:21:04.130680Z","iopub.status.idle":"2025-06-01T11:21:04.275947Z","shell.execute_reply.started":"2025-06-01T11:21:04.130655Z","shell.execute_reply":"2025-06-01T11:21:04.275093Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Preprocessing\ndef clean_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    text = re.sub(r'\\W', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndf['text'] = df['text'].apply(clean_text)\n\n# Remove empty texts\ndf = df[df['text'].str.strip() != '']\ndf.dropna(subset=['text', 'spam'], inplace=True)\n\nprint(df['text'].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:22:49.526582Z","iopub.execute_input":"2025-06-01T11:22:49.527033Z","iopub.status.idle":"2025-06-01T11:22:49.637845Z","shell.execute_reply.started":"2025-06-01T11:22:49.527011Z","shell.execute_reply":"2025-06-01T11:22:49.637066Z"}},"outputs":[{"name":"stdout","text":"0    0\n1    0\n2    0\n3    0\n4    0\nName: text, dtype: object\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:23:29.016475Z","iopub.execute_input":"2025-06-01T11:23:29.016749Z","iopub.status.idle":"2025-06-01T11:23:29.020573Z","shell.execute_reply.started":"2025-06-01T11:23:29.016729Z","shell.execute_reply":"2025-06-01T11:23:29.019872Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print(\"First 10 rows of text column:\\n\", df['text'].head(10))\nprint(\"Data type of text column:\", df['text'].dtype)\nprint(\"Number of missing values:\", df['text'].isna().sum())\nprint(\"Number of empty strings:\", df['text'].str.strip().eq('').sum())\nprint(\"Sample of non-empty text:\", df['text'][df['text'].str.strip() != ''].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:26:40.967797Z","iopub.execute_input":"2025-06-01T11:26:40.968558Z","iopub.status.idle":"2025-06-01T11:26:40.978591Z","shell.execute_reply.started":"2025-06-01T11:26:40.968534Z","shell.execute_reply":"2025-06-01T11:26:40.977738Z"}},"outputs":[{"name":"stdout","text":"First 10 rows of text column:\n 0    0\n1    0\n2    0\n3    0\n4    0\n5    1\n6    0\n7    0\n8    0\n9    0\nName: text, dtype: object\nData type of text column: object\nNumber of missing values: 0\nNumber of empty strings: 0\nSample of non-empty text: 0    0\n1    0\n2    0\n3    0\n4    0\nName: text, dtype: object\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Replace 'message' with the actual column name containing text\ntext_column = 'message'  # Update this based on df.columns output\nif text_column not in df.columns:\n    raise ValueError(f\"Column '{text_column}' not found. Available columns: {df.columns}\")\n\n# Ensure text is string and handle missing values\ndf[text_column] = df[text_column].astype(str).fillna('')\n\n# Check for empty text\nif df[text_column].str.strip().eq('').all():\n    raise ValueError(f\"All entries in '{text_column}' are empty. Check your data:\\n{df[text_column].head()}\")\n\n# TF-IDF Vectorization\nvectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\nX = vectorizer.fit_transform(df[text_column])\n\n# Labels (use the 'spam' column)\ny = df['spam']\n\nprint(\"Vectorized data shape:\", X.shape)\nprint(\"Sample features:\", vectorizer.get_feature_names_out()[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:27:32.488866Z","iopub.execute_input":"2025-06-01T11:27:32.489347Z","iopub.status.idle":"2025-06-01T11:27:32.564772Z","shell.execute_reply.started":"2025-06-01T11:27:32.489323Z","shell.execute_reply":"2025-06-01T11:27:32.564154Z"}},"outputs":[{"name":"stdout","text":"Vectorized data shape: (5172, 4)\nSample features: ['11' '13' '15' '16']\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Train/Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Naive Bayes model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:29:32.417839Z","iopub.execute_input":"2025-06-01T11:29:32.418324Z","iopub.status.idle":"2025-06-01T11:29:32.424706Z","shell.execute_reply.started":"2025-06-01T11:29:32.418300Z","shell.execute_reply":"2025-06-01T11:29:32.423937Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Train Naive Bayes model\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:29:56.870290Z","iopub.execute_input":"2025-06-01T11:29:56.870788Z","iopub.status.idle":"2025-06-01T11:29:56.878905Z","shell.execute_reply.started":"2025-06-01T11:29:56.870764Z","shell.execute_reply":"2025-06-01T11:29:56.878038Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"MultinomialNB()","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Predictions\ny_pred = model.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:30:14.844427Z","iopub.execute_input":"2025-06-01T11:30:14.845130Z","iopub.status.idle":"2025-06-01T11:30:14.848868Z","shell.execute_reply.started":"2025-06-01T11:30:14.845106Z","shell.execute_reply":"2025-06-01T11:30:14.848276Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Evaluation\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"Accuracy Score:\", accuracy_score(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:30:27.755282Z","iopub.execute_input":"2025-06-01T11:30:27.755555Z","iopub.status.idle":"2025-06-01T11:30:27.771941Z","shell.execute_reply.started":"2025-06-01T11:30:27.755536Z","shell.execute_reply":"2025-06-01T11:30:27.771222Z"}},"outputs":[{"name":"stdout","text":"Confusion Matrix:\n [[1024    0]\n [  11    0]]\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1024\n           1       0.00      0.00      0.00        11\n\n    accuracy                           0.99      1035\n   macro avg       0.49      0.50      0.50      1035\nweighted avg       0.98      0.99      0.98      1035\n\nAccuracy Score: 0.9893719806763285\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import joblib\n\n# Save the trained Naive Bayes model\njoblib.dump(model, '/kaggle/working/email_spam_model.pkl')\n\n# Save the TF-IDF vectorizer\njoblib.dump(vectorizer, '/kaggle/working/tfidf_vectorizer.pkl')\n\nprint(\"Model and vectorizer saved to /kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:31:41.518085Z","iopub.execute_input":"2025-06-01T11:31:41.518378Z","iopub.status.idle":"2025-06-01T11:31:41.524286Z","shell.execute_reply.started":"2025-06-01T11:31:41.518358Z","shell.execute_reply":"2025-06-01T11:31:41.523675Z"}},"outputs":[{"name":"stdout","text":"Model and vectorizer saved to /kaggle/working/\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset (adjust path if needed)\ndf = pd.read_csv('/kaggle/input/email-spam-classification-dataset-csv/emails.csv', encoding='latin-1')\nprint(\"Column names:\", df.columns)\nprint(\"First 5 rows:\\n\", df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:35:38.356904Z","iopub.execute_input":"2025-06-01T11:35:38.357431Z","iopub.status.idle":"2025-06-01T11:35:39.332846Z","shell.execute_reply.started":"2025-06-01T11:35:38.357406Z","shell.execute_reply":"2025-06-01T11:35:39.332051Z"}},"outputs":[{"name":"stdout","text":"Column names: Index(['Email No.', 'the', 'to', 'ect', 'and', 'for', 'of', 'a', 'you', 'hou',\n       ...\n       'connevey', 'jay', 'valued', 'lay', 'infrastructure', 'military',\n       'allowing', 'ff', 'dry', 'Prediction'],\n      dtype='object', length=3002)\nFirst 5 rows:\n   Email No.  the  to  ect  and  for  of    a  you  hou  ...  connevey  jay  \\\n0   Email 1    0   0    1    0    0   0    2    0    0  ...         0    0   \n1   Email 2    8  13   24    6    6   2  102    1   27  ...         0    0   \n2   Email 3    0   0    1    0    0   0    8    0    0  ...         0    0   \n3   Email 4    0   5   22    0    5   1   51    2   10  ...         0    0   \n4   Email 5    7   6   17    1    5   2   57    0    9  ...         0    0   \n\n   valued  lay  infrastructure  military  allowing  ff  dry  Prediction  \n0       0    0               0         0         0   0    0           0  \n1       0    0               0         0         0   1    0           0  \n2       0    0               0         0         0   0    0           0  \n3       0    0               0         0         0   0    0           0  \n4       0    0               0         0         0   1    0           0  \n\n[5 rows x 3002 columns]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!pip install gradio --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:36:25.754276Z","iopub.execute_input":"2025-06-01T11:36:25.754549Z","iopub.status.idle":"2025-06-01T11:36:35.644707Z","shell.execute_reply.started":"2025-06-01T11:36:25.754529Z","shell.execute_reply":"2025-06-01T11:36:35.643916Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import pandas as pd\nimport joblib\nimport pickle\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nimport gradio as gr\nimport re\nimport string\n\n# Initialize model and vectorizer as None\nmodel = None\nvectorizer = None\n\ndef preprocess_text(text):\n    \"\"\"Clean and preprocess email text\"\"\"\n    if not text:\n        return \"\"\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    \n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    \n    return text\n\ndef load_or_create_model():\n    \"\"\"Load existing model or create a new one\"\"\"\n    global model, vectorizer\n    \n    try:\n        # Try to load pre-trained model and vectorizer\n        model = joblib.load('/kaggle/input/email-spam/keras/default/1/email_spam_model.pkl')\n        vectorizer = joblib.load('/kaggle/input/email-spam/keras/default/1/tfidf_vectorizer.pkl')\n        print(\"✅ Loaded pre-trained model and vectorizer\")\n        return True\n    except:\n        print(\"⚠️ Pre-trained model not found, creating new model...\")\n        \n    try:\n        # Load and prepare dataset\n        df = pd.read_csv('/kaggle/input/email-spam/spam.csv', encoding='latin-1')\n        \n        # Assume the dataset has columns like 'v1' (label) and 'v2' (text)\n        # Adjust column names based on your actual dataset structure\n        if 'v1' in df.columns and 'v2' in df.columns:\n            df = df[['v1', 'v2']].dropna()\n            df.columns = ['label', 'text']\n        elif 'Prediction' in df.columns:\n            # If your dataset has different structure\n            text_cols = [col for col in df.columns if col not in ['Email No.', 'Prediction']]\n            if text_cols:\n                df['text'] = df[text_cols].astype(str).agg(' '.join, axis=1)\n                df['label'] = df['Prediction'].map({1: 'spam', 0: 'ham'})\n                df = df[['label', 'text']].dropna()\n        else:\n            raise ValueError(\"Unable to identify text and label columns in dataset\")\n        \n        # Preprocess text\n        df['text'] = df['text'].apply(preprocess_text)\n        \n        # Create and train model\n        vectorizer = TfidfVectorizer(\n            max_features=5000,\n            stop_words='english',\n            ngram_range=(1, 2),\n            max_df=0.95,\n            min_df=2\n        )\n        \n        # Create pipeline\n        model = Pipeline([\n            ('tfidf', vectorizer),\n            ('classifier', MultinomialNB(alpha=0.1))\n        ])\n        \n        # Train model\n        X_train, X_test, y_train, y_test = train_test_split(\n            df['text'], df['label'], test_size=0.2, random_state=42\n        )\n        \n        model.fit(X_train, y_train)\n        \n        # Get accuracy\n        accuracy = model.score(X_test, y_test)\n        print(f\"✅ Model trained successfully! Accuracy: {accuracy:.3f}\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"❌ Error creating model: {str(e)}\")\n        return False\n\ndef create_fallback_classifier():\n    \"\"\"Create a simple keyword-based classifier as fallback\"\"\"\n    global model, vectorizer\n    \n    # High-confidence spam indicators\n    high_spam_keywords = [\n        'congratulations', 'winner', 'won', 'prize', 'lottery', 'jackpot',\n        'urgent', 'act now', 'limited time', 'expires', 'hurry',\n        'click here', 'click now', 'claim now', 'visit now',\n        'free money', 'easy money', 'make money', 'earn money',\n        'viagra', 'cialis', 'pharmacy', 'medication', 'pills',\n        'hot singles', 'meet singles', 'dating', 'lonely', 'romance',\n        'nigerian prince', 'inheritance', 'million dollars'\n    ]\n    \n    # Medium spam indicators\n    medium_spam_keywords = [\n        'free', 'offer', 'deal', 'discount', 'sale', 'promotion',\n        'limited', 'exclusive', 'special', 'bonus', 'gift',\n        'money', 'cash', 'earn', 'income', 'profit', 'guarantee',\n        'loan', 'credit', 'debt', 'refinance', 'mortgage',\n        'weight loss', 'lose weight', 'diet', 'supplements',\n        'casino', 'gambling', 'bet', 'lottery', 'scratch',\n        'work from home', 'make money online', 'business opportunity'\n    ]\n    \n    # Spam phrases (exact matches)\n    spam_phrases = [\n        'hot singles in your area', 'meet singles near you',\n        'you have won', 'you are a winner', 'claim your prize',\n        'act now', 'limited time offer', 'expires soon',\n        'click here now', 'visit our website', 'call now',\n        'no credit check', 'guaranteed approval', 'risk free',\n        'work from home', 'make money fast', 'easy money'\n    ]\n    \n    def fallback_predict(text):\n        if not text:\n            return \"ham\"\n        \n        text_lower = text.lower().strip()\n        \n        # Check for exact spam phrases first\n        for phrase in spam_phrases:\n            if phrase in text_lower:\n                return \"spam\"\n        \n        # Count high-confidence spam keywords (weight = 3)\n        high_spam_score = sum(3 for keyword in high_spam_keywords if keyword in text_lower)\n        \n        # Count medium-confidence spam keywords (weight = 1)\n        medium_spam_score = sum(1 for keyword in medium_spam_keywords if keyword in text_lower)\n        \n        total_spam_score = high_spam_score + medium_spam_score\n        \n        # Classification logic\n        if high_spam_score >= 3:  # Any high-confidence keyword = spam\n            return \"spam\"\n        elif total_spam_score >= 4:  # Multiple medium keywords = spam\n            return \"spam\"\n        elif total_spam_score >= 2 and any(urgent in text_lower for urgent in ['urgent', '!', 'now', 'immediately']):\n            return \"spam\"  # Medium score + urgency = spam\n        else:\n            return \"ham\"\n    \n    model = fallback_predict\n    vectorizer = None\n    print(\"✅ Created improved fallback keyword-based classifier\")\n\ndef predict_email(email_text):\n    \"\"\"Predict if email is spam or ham\"\"\"\n    global model, vectorizer\n    \n    # Initialize model if not done already\n    if model is None:\n        success = load_or_create_model()\n        if not success:\n            create_fallback_classifier()\n    \n    # Handle empty input\n    if not email_text or email_text.strip() == \"\":\n        return \"📝 Please enter an email or message to classify.\"\n    \n    try:\n        # Preprocess input\n        cleaned_text = preprocess_text(email_text)\n        \n        if hasattr(model, 'predict'):\n            # Scikit-learn pipeline model\n            prediction = model.predict([cleaned_text])[0]\n            \n            # Get probability if available\n            if hasattr(model, 'predict_proba'):\n                proba = model.predict_proba([cleaned_text])[0]\n                if prediction == 'spam':\n                    confidence = max(proba)\n                    return f\"🚨 **SPAM** (ML Confidence: {confidence:.1%})\"\n                else:\n                    confidence = max(proba)\n                    return f\"✅ **HAM** (ML Confidence: {confidence:.1%})\"\n            else:\n                return f\"🚨 **SPAM** (ML Model)\" if prediction == 'spam' else \"✅ **HAM** (ML Model)\"\n        else:\n            # Fallback classifier with detailed reasoning\n            prediction = model(email_text)\n            \n            # Provide reasoning for keyword-based classification\n            text_lower = email_text.lower()\n            detected_keywords = []\n            \n            # Check what triggered the classification\n            high_spam_keywords = [\n                'congratulations', 'winner', 'won', 'prize', 'hot singles',\n                'urgent', 'act now', 'click here', 'free money', 'viagra'\n            ]\n            medium_spam_keywords = [\n                'free', 'offer', 'money', 'deal', 'limited', 'exclusive'\n            ]\n            \n            for keyword in high_spam_keywords:\n                if keyword in text_lower:\n                    detected_keywords.append(f\"'{keyword}' (high)\")\n            \n            for keyword in medium_spam_keywords:\n                if keyword in text_lower and keyword not in ' '.join(detected_keywords):\n                    detected_keywords.append(f\"'{keyword}' (med)\")\n                    \n            if prediction == 'spam':\n                if detected_keywords:\n                    keywords_text = ', '.join(detected_keywords[:3])  # Show max 3\n                    return f\"🚨 **SPAM** (Keywords: {keywords_text})\"\n                else:\n                    return f\"🚨 **SPAM** (Pattern-based)\"\n            else:\n                return f\"✅ **HAM** (No spam indicators found)\"\n            \n    except Exception as e:\n        # Last resort: simple keyword check with error handling\n        spam_indicators = ['win', 'free', 'prize', 'click', 'offer', 'urgent', 'congratulations', 'hot singles']\n        detected = [word for word in spam_indicators if word in email_text.lower()]\n        \n        if detected:\n            return f\"🚨 **SPAM** (Detected: {', '.join(detected[:2])} - Backup mode)\"\n        else:\n            return f\"✅ **HAM** (Backup mode - Error: {str(e)[:30]}...)\"\n\n# Example texts for testing\nexample_texts = [\n    \"Congratulations! You've won a $1000 Walmart gift card. Click here to claim now!\",\n    \"Hey, how are you doing? Want to meet for coffee later?\",\n    \"URGENT: Your account will be suspended. Click here immediately!\",\n    \"Hi mom, just wanted to let you know I arrived safely.\",\n    \"FREE VIAGRA! Limited time offer! Act now!\",\n    \"Meeting scheduled for tomorrow at 2 PM in conference room A.\"\n]\n\n# Create Gradio interface\niface = gr.Interface(\n    fn=predict_email,\n    inputs=gr.Textbox(\n        lines=4,\n        placeholder=\"Enter your email content here...\\n\\nExample: 'Congratulations! You've won a prize!' or 'Hi, how are you?'\",\n        label=\"Email Content\"\n    ),\n    outputs=gr.Textbox(label=\"Classification Result\"),\n    title=\"📧 Email Spam Classifier\",\n    description=\"\"\"\n    **Enter any email content to check if it's Spam or Ham (legitimate email).**\n    \n    The classifier uses machine learning to analyze text patterns and identify spam characteristics.\n    \n    🚨 **Spam**: Promotional, suspicious, or unwanted emails  \n    ✅ **Ham**: Legitimate, normal emails\n    \"\"\",\n    examples=example_texts,\n    theme=gr.themes.Soft(),\n    allow_flagging=\"never\"\n)\n\n# Launch the interface\nif __name__ == \"__main__\":\n    print(\"🚀 Starting Email Spam Classifier...\")\n    iface.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T12:13:13.869868Z","iopub.execute_input":"2025-06-01T12:13:13.870208Z","iopub.status.idle":"2025-06-01T12:13:14.861753Z","shell.execute_reply.started":"2025-06-01T12:13:13.870172Z","shell.execute_reply":"2025-06-01T12:13:14.861229Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"🚀 Starting Email Spam Classifier...\n* Running on local URL:  http://127.0.0.1:7867\n* Running on public URL: https://570d7c2fba26e91386.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://570d7c2fba26e91386.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"⚠️ Pre-trained model not found, creating new model...\n❌ Error creating model: [Errno 2] No such file or directory: '/kaggle/input/email-spam/spam.csv'\n✅ Created improved fallback keyword-based classifier\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import pandas as pd\nimport joblib\nimport pickle\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nimport gradio as gr\nimport re\nimport string\nimport random\n\n# Initialize model and vectorizer as None\nmodel = None\nvectorizer = None\n\ndef preprocess_text(text):\n    \"\"\"Clean and preprocess text\"\"\"\n    if not text:\n        return \"\"\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove special characters and digits but keep basic punctuation\n    text = re.sub(r'[^\\w\\s!?.,]', '', text)\n    \n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    \n    return text\n\ndef create_smart_classifier():\n    \"\"\"Create an enhanced keyword and pattern-based classifier\"\"\"\n    global model, vectorizer\n    \n    # Enhanced spam indicators with weights\n    spam_patterns = {\n        # High confidence spam indicators (weight: 5)\n        'high_confidence': [\n            'congratulations', 'winner', 'won', 'prize', 'lottery', 'jackpot',\n            'nigerian prince', 'inheritance', 'million dollars', 'bank transfer',\n            'hot singles', 'meet singles', 'lonely tonight', 'sexy',\n            'viagra', 'cialis', 'pharmacy', 'medication', 'pills',\n            'click here now', 'claim now', 'act immediately', 'limited time',\n            'you have been selected', 'final notice', 'urgent response required'\n        ],\n        \n        # Medium confidence spam indicators (weight: 3)\n        'medium_confidence': [\n            'free money', 'easy money', 'make money', 'earn money', 'quick cash',\n            'work from home', 'business opportunity', 'guaranteed income',\n            'lose weight fast', 'weight loss', 'diet pills', 'supplements',\n            'casino', 'gambling', 'bet now', 'lottery ticket',\n            'no credit check', 'guaranteed approval', 'loan approved',\n            'refinance', 'mortgage', 'debt relief', 'consolidation',\n            'risk free', 'money back guarantee', 'no obligation'\n        ],\n        \n        # Lower confidence spam indicators (weight: 2)\n        'low_confidence': [\n            'free', 'offer', 'deal', 'discount', 'sale', 'promotion',\n            'limited', 'exclusive', 'special', 'bonus', 'gift',\n            'urgent', 'hurry', 'expires', 'deadline', 'now',\n            'click', 'visit', 'call', 'order', 'buy',\n            'money', 'cash', 'earn', 'profit', 'save'\n        ]\n    }\n    \n    # Spam phrases that are strong indicators\n    spam_phrases = [\n        'hot singles in your area', 'meet singles near you',\n        'you have won', 'you are a winner', 'claim your prize',\n        'act now or lose', 'limited time offer', 'expires today',\n        'click here now', 'visit our website now', 'call now',\n        'no credit check required', 'guaranteed approval',\n        'work from home', 'make money fast', 'easy money online',\n        'lose weight without', 'diet pills that work',\n        'nigerian prince needs help', 'millions of dollars'\n    ]\n    \n    # Legitimate patterns (negative indicators for spam)\n    legitimate_patterns = [\n        'meeting', 'conference', 'schedule', 'appointment',\n        'family', 'friend', 'mom', 'dad', 'brother', 'sister',\n        'work', 'office', 'project', 'team', 'colleague',\n        'thank you', 'please', 'regards', 'sincerely',\n        'birthday', 'anniversary', 'celebration', 'holiday'\n    ]\n    \n    def enhanced_predict(text):\n        if not text or len(text.strip()) < 3:\n            return \"ham\", 0, []\n        \n        text_lower = text.lower().strip()\n        detected_patterns = []\n        spam_score = 0\n        \n        # Check for exact spam phrases first (highest weight)\n        for phrase in spam_phrases:\n            if phrase in text_lower:\n                spam_score += 8\n                detected_patterns.append(f\"'{phrase}' (phrase)\")\n        \n        # Check high confidence keywords\n        for keyword in spam_patterns['high_confidence']:\n            if keyword in text_lower:\n                spam_score += 5\n                detected_patterns.append(f\"'{keyword}' (high)\")\n        \n        # Check medium confidence keywords\n        for keyword in spam_patterns['medium_confidence']:\n            if keyword in text_lower:\n                spam_score += 3\n                detected_patterns.append(f\"'{keyword}' (medium)\")\n        \n        # Check low confidence keywords\n        for keyword in spam_patterns['low_confidence']:\n            if keyword in text_lower:\n                spam_score += 2\n                detected_patterns.append(f\"'{keyword}' (low)\")\n        \n        # Check for legitimate patterns (reduce spam score)\n        legitimate_count = 0\n        for pattern in legitimate_patterns:\n            if pattern in text_lower:\n                legitimate_count += 1\n        \n        # Reduce spam score for legitimate patterns\n        spam_score -= legitimate_count * 1.5\n        \n        # Additional pattern checks\n        exclamation_count = text.count('!')\n        caps_ratio = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n        \n        # Excessive exclamation marks or caps\n        if exclamation_count >= 3:\n            spam_score += 2\n            detected_patterns.append(\"multiple exclamations\")\n        \n        if caps_ratio > 0.3 and len(text) > 10:\n            spam_score += 2\n            detected_patterns.append(\"excessive caps\")\n        \n        # URL detection\n        if 'http' in text_lower or 'www.' in text_lower or '.com' in text_lower:\n            spam_score += 1\n            detected_patterns.append(\"contains URL\")\n        \n        # Determine classification\n        if spam_score >= 8:\n            return \"spam\", min(spam_score * 10, 95), detected_patterns\n        elif spam_score >= 5:\n            return \"spam\", min(spam_score * 8, 85), detected_patterns\n        elif spam_score >= 3:\n            return \"spam\", min(spam_score * 12, 75), detected_patterns\n        else:\n            return \"ham\", max(10, 100 - spam_score * 5), detected_patterns\n    \n    model = enhanced_predict\n    vectorizer = None\n    print(\"✅ Created enhanced pattern-based classifier\")\n\ndef predict_text(input_text):\n    \"\"\"Predict if any text is spam-like or legitimate\"\"\"\n    global model\n    \n    # Initialize model if not done already\n    if model is None:\n        create_smart_classifier()\n    \n    # Handle empty input\n    if not input_text or input_text.strip() == \"\":\n        return \"📝 Please enter some text to classify.\"\n    \n    try:\n        # Get prediction with confidence and detected patterns\n        prediction, confidence, detected_patterns = model(input_text)\n        \n        # Format the result\n        if prediction == 'spam':\n            result = f\"🚨 **SPAM-LIKE** (Confidence: {confidence}%)\"\n        else:\n            result = f\"✅ **LEGITIMATE** (Confidence: {confidence}%)\"\n        \n        # Add detected patterns if any\n        if detected_patterns:\n            patterns_text = \", \".join(detected_patterns[:4])  # Show max 4 patterns\n            result += f\"\\n\\n🔍 **Detected patterns:** {patterns_text}\"\n            if len(detected_patterns) > 4:\n                result += f\" (+{len(detected_patterns)-4} more)\"\n        \n        # Add explanation\n        if prediction == 'spam':\n            result += \"\\n\\n⚠️ This text contains characteristics commonly found in spam or promotional content.\"\n        else:\n            result += \"\\n\\n✨ This text appears to be legitimate and natural.\"\n            \n        return result\n        \n    except Exception as e:\n        return f\"❌ Error analyzing text: {str(e)}\"\n\ndef generate_random_sentence():\n    \"\"\"Generate a random sentence for testing\"\"\"\n    sentences = [\n        # Legitimate sentences\n        \"Hey, how are you doing today?\",\n        \"The meeting is scheduled for tomorrow at 3 PM.\",\n        \"Thanks for your help with the project yesterday.\",\n        \"Happy birthday! Hope you have a wonderful day.\",\n        \"Can you please send me the report by Friday?\",\n        \"I'll be working from home tomorrow.\",\n        \"Let's grab coffee sometime this week.\",\n        \"The weather is really nice today, isn't it?\",\n        \"Please review the attached document when you have time.\",\n        \"Looking forward to seeing you at the conference.\",\n        \n        # Spam-like sentences\n        \"Congratulations! You've won a $1000 gift card!\",\n        \"URGENT: Click here to claim your prize now!\",\n        \"Make money fast with this amazing opportunity!\",\n        \"Hot singles in your area want to meet you!\",\n        \"FREE VIAGRA! Limited time offer, act now!\",\n        \"You have been selected for a special promotion!\",\n        \"Lose weight fast with these miracle pills!\",\n        \"FINAL NOTICE: Your account will be suspended!\",\n        \"Nigerian prince needs your help with millions!\",\n        \"Guaranteed loan approval, no credit check required!\",\n        \n        # Mixed/borderline sentences\n        \"Special offer just for you - 50% off everything!\",\n        \"Don't miss out on this limited time deal!\",\n        \"Free shipping on your next order!\",\n        \"Urgent: Please update your account information.\",\n        \"Click here to unsubscribe from our mailing list.\"\n    ]\n    \n    return random.choice(sentences)\n\n# Create Gradio interface\nwith gr.Blocks(theme=gr.themes.Soft(), title=\"Universal Text Spam Classifier\") as iface:\n    gr.Markdown(\"\"\"\n    # 🤖 Universal Text Spam Classifier\n    \n    **Analyze any text to detect spam-like characteristics!**\n    \n    This classifier can analyze emails, messages, social media posts, or any text content to identify:\n    - 🚨 **Spam-like content**: Promotional, suspicious, or unwanted text\n    - ✅ **Legitimate content**: Natural, normal communication\n    \n    Enter any text below or try the random sentence generator!\n    \"\"\")\n    \n    with gr.Row():\n        with gr.Column(scale=3):\n            text_input = gr.Textbox(\n                lines=4,\n                placeholder=\"Enter any text here...\\n\\nExamples:\\n• 'Hey, how are you?'\\n• 'Congratulations! You won!'\\n• 'Meeting at 3 PM tomorrow'\",\n                label=\"Text to Analyze\"\n            )\n        \n        with gr.Column(scale=1):\n            random_btn = gr.Button(\"🎲 Generate Random Sentence\", variant=\"secondary\")\n    \n    classify_btn = gr.Button(\"🔍 Analyze Text\", variant=\"primary\", size=\"lg\")\n    \n    result_output = gr.Textbox(\n        label=\"Analysis Result\",\n        lines=6,\n        interactive=False\n    )\n    \n    gr.Markdown(\"\"\"\n    ### 📚 How it works:\n    - **Pattern Recognition**: Identifies common spam keywords and phrases\n    - **Context Analysis**: Considers legitimate communication patterns\n    - **Confidence Scoring**: Provides confidence levels for classifications\n    - **Multi-factor Detection**: Analyzes capitalization, punctuation, and URL patterns\n    \n    ### 🎯 Use Cases:\n    - Email filtering and security\n    - Social media content moderation\n    - Message screening\n    - Content quality assessment\n    \"\"\")\n    \n    # Event handlers\n    classify_btn.click(\n        fn=predict_text,\n        inputs=text_input,\n        outputs=result_output\n    )\n    \n    random_btn.click(\n        fn=generate_random_sentence,\n        outputs=text_input\n    )\n    \n    text_input.submit(\n        fn=predict_text,\n        inputs=text_input,\n        outputs=result_output\n    )\n\n# Launch the interface\nif __name__ == \"__main__\":\n    print(\"🚀 Starting Universal Text Spam Classifier...\")\n    iface.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T12:20:44.615409Z","iopub.execute_input":"2025-06-01T12:20:44.615696Z","iopub.status.idle":"2025-06-01T12:20:45.539741Z","shell.execute_reply.started":"2025-06-01T12:20:44.615677Z","shell.execute_reply":"2025-06-01T12:20:45.539229Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Universal Text Spam Classifier...\n* Running on local URL:  http://127.0.0.1:7868\n* Running on public URL: https://368b907736658570da.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://368b907736658570da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"✅ Created enhanced pattern-based classifier\n","output_type":"stream"}],"execution_count":41}]}